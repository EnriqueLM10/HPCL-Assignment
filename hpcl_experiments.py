# -*- coding: utf-8 -*-
"""HPCL Experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d1XrtJSV50-JU7f7bdaFCIScacDm2E5z

**Experiments 3-14**

Experiment 3: OpenMP Program to Print "Hello World" with Thread ID
"""

#include <stdio.h>
#include <omp.h>

int main() {
    #pragma omp parallel
    {
        printf("Hello World from thread %d\n", omp_get_thread_num());
    }
    return 0;
}

"""Experiment 4: Print Your Name from 4 Underlying Cores


"""

#include <stdio.h>
#include <omp.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        int id = omp_get_thread_num();
        printf("Thread %d says: Hello, I am [Your Name]!\n", id);
    }
    return 0;
}

"""Experiment 5: OpenMP Program to Demonstrate firstprivate

"""

#include <stdio.h>
#include <omp.h>

int main() {
    int val = 1234;
    printf("Initial value of val: %d\n", val);

    #pragma omp parallel firstprivate(val) num_threads(4)
    {
        printf("Thread %d, initial val = %d\n", omp_get_thread_num(), val);
        val += 1;
        printf("Thread %d, updated val = %d\n", omp_get_thread_num(), val);
    }

    printf("Final value of val: %d\n", val);
    return 0;
}

"""Experiment 6: OpenMP Program to Demonstrate private"""

#include <stdio.h>
#include <omp.h>

int main() {
    int val = 0;

    #pragma omp parallel private(val) num_threads(4)
    {
        printf("Thread %d, initial val = %d\n", omp_get_thread_num(), val);
        val = omp_get_thread_num();
        printf("Thread %d, updated val = %d\n", omp_get_thread_num(), val);
    }

    return 0;
}

"""Experiment 7: Static Scheduling with OpenMP"""

#include <stdio.h>
#include <omp.h>

#define N 16
#define CHUNK 4

int main() {
    int i;
    int array[N];

    #pragma omp parallel for schedule(static, CHUNK) num_threads(4)
    for (i = 0; i < N; i++) {
        array[i] = i * 2;
        printf("Thread %d processes index %d\n", omp_get_thread_num(), i);
    }

    return 0;
}

"""Experiment 8: Printing Series 2 and 4 in Parallel Threads"""

#include <stdio.h>
#include <omp.h>

int main() {
    #pragma omp parallel sections
    {
        #pragma omp section
        {
            printf("Thread %d: Series 2\n", omp_get_thread_num());
        }
        #pragma omp section
        {
            printf("Thread %d: Series 4\n", omp_get_thread_num());
        }
    }
    return 0;
}

"""Experiment 9: MPI Program to Print "Hello World"
"""

#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    printf("Hello World from process %d\n", rank);
    MPI_Finalize();
    return 0;
}

"""Experiment 9: MPI Program to Send and Receive "Hello World"
"""

#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        char msg[20];
        for (int i = 1; i < size; i++) {
            MPI_Recv(msg, 20, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            printf("Root received: %s\n", msg);
        }
    } else {
        char msg[20] = "Hello World";
        MPI_Send(msg, 20, MPI_CHAR, 0, 0, MPI_COMM_WORLD);
    }

    MPI_Finalize();
    return 0;
}

"""Experiment 10: MPI Program to Send Two Numbers per Process to Root"""

#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int nums[2] = {rank * 2, rank * 2 + 1};
    if (rank == 0) {
        int recv_nums[2 * (size - 1)];
        for (int i = 1; i < size; i++) {
            MPI_Recv(&recv_nums[2 * (i - 1)], 2, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        for (int i = 0; i < 2 * (size - 1); i += 2) {
            printf("Root received: %d, %d\n", recv_nums[i], recv_nums[i + 1]);
        }
    } else {
        MPI_Send(nums, 2, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }

    MPI_Finalize();
    return 0;
}

"""Experiment 11: MPI Program to Sum First N Integers"""

#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, size, n = 10000;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int chunk = n / size;
    int start = rank * chunk + 1;
    int end = start + chunk - 1;

    int local_sum = 0;
    for (int i = start; i <= end; i++) {
        local_sum += i;
    }

    int total_sum = 0;
    MPI_Reduce(&local_sum, &total_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Sum of first %d integers: %d\n", n, total_sum);
    }

    MPI_Finalize();
    return 0;
}

"""Experiment 12: MPI Program Using Ring Topology for Sum"""

#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, size, n = 10, value = 0;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        value = n;
        MPI_Send(&value, 1, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD);
        MPI_Recv(&value, 1, MPI_INT, (rank - 1 + size) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Final result at process %d: %d\n", rank, value);
    } else {
        MPI_Recv(&value, 1, MPI_INT, (rank - 1 + size) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        value += n;
        MPI_Send(&value, 1, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD);
    }

    MPI_Finalize();
    return 0;
}

"""Experiment 13: CUDA Program for Matrix Addition"""

#include <stdio.h>
#include <cuda.h>

__global__ void addMatrices(int *a, int *b, int *c, int n) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

int main() {
    int n = 16;
    int size = n * sizeof(int);
    int a[16], b[16], c[16];

    for (int i = 0; i < n; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    int *d_a, *d_b, *d_c;
    cudaMalloc((void**)&d_a, size);
    cudaMalloc((void**)&d_b, size);
    cudaMalloc((void**)&d_c, size);

    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);

    addMatrices<<<2, 8>>>(d_a, d_b, d_c, n);

    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);

    for (int i = 0; i < n; i++) {
        printf("c[%d] = %d\n", i, c[i]);
    }

    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}

"""Experiment 14: CUDA Program for Matrix Multiplication"""

#include <stdio.h>
#include <cuda.h>

__global__ void multiplyMatrices(int *a, int *b, int *c, int n) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < n && col < n) {
        int sum = 0;
        for (int k = 0; k < n; k++) {
            sum += a[row * n + k] * b[k * n + col];
        }
        c[row * n + col] = sum;
    }
}

int main() {
    int n = 2;
    int size = n * n * sizeof(int);
    int a[4] = {1, 2, 3, 4};
    int b[4] = {5, 6, 7, 8};
    int c[4] = {0};

    int *d_a, *d_b, *d_c;
    cudaMalloc((void**)&d_a, size);
    cudaMalloc((void**)&d_b, size);
    cudaMalloc((void**)&d_c, size);

    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(2, 2);
    dim3 blocksPerGrid(1, 1);

    multiplyMatrices<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);

    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);

    for (int i = 0; i < n * n; i++) {
        printf("c[%d] = %d\n", i, c[i]);
    }

    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}